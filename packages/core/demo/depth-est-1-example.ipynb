{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c89c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing depth estimation speed on a subset of ImageNet made of 1 class (n15075141)\n",
    "# 1300 elements\n",
    "input_folder = \"image-net/ILSVRC2012_img_val\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19956c86",
   "metadata": {},
   "source": [
    "## Load Marigold Pipeline with Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "import torch\n",
    "from diffusers.models.attention_processor import AttnProcessor2_0\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "pipe = diffusers.MarigoldDepthPipeline.from_pretrained(\n",
    "    \"prs-eth/marigold-depth-v1-1\", variant=\"fp16\", #torch_dtype=torch.float16 # commented this to make it run smoothly on Apple Silicon M1+\n",
    ").to(device)\n",
    "\n",
    "# pipe.vae = diffusers.AutoencoderTiny.from_pretrained(\n",
    "#     \"madebyollin/taesd\", torch_dtype=torch.float16\n",
    "# ).cuda()\n",
    "\n",
    "# pipe.vae.set_attn_processor(AttnProcessor2_0()) \n",
    "# pipe.unet.set_attn_processor(AttnProcessor2_0())\n",
    "\n",
    "# pipe.vae = torch.compile(pipe.vae, mode=\"reduce-overhead\", fullgraph=True)\n",
    "# pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Get list of image files from the folder\n",
    "image_files = [f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.jpeg', '.JPEG', '.JPG'))]\n",
    "\n",
    "# Select random image\n",
    "image_path = os.path.join(input_folder, random.choice(image_files))\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(image_path)\n",
    "print(f\"Loaded image from: {image_path}\")\n",
    "print(f\"Image size: {image.size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Random Image\")\n",
    "plt.show()\n",
    "\n",
    "image.save(\"image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb18cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_est = pipe(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_image = pipe.image_processor.visualize_depth(depth_est.prediction)[0]\n",
    "\n",
    "plt.imshow(depth_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Depth Estimation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27550507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The 16-bit PNG file stores the single channel values mapped linearly from the [0, 1] range into [0, 65535]\n",
    "visualized_depth = pipe.image_processor.export_depth_to_16bit_png(\n",
    "    depth_est.prediction[0]\n",
    ")\n",
    "\n",
    "visualized_depth[0].save(\"depth_estimation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a0b47",
   "metadata": {},
   "source": [
    "Color map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cf086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vcs2425 import colormap\n",
    "\n",
    "i = Image.open(\"depth_estimation.png\")\n",
    "colormaps = ['viridis', 'Spectral', 'plasma', 'gray']\n",
    "\n",
    "for cmap in colormaps:\n",
    "    ci = colormap(np.array(i), cmap)\n",
    "    plt.imshow(ci)\n",
    "    plt.title(f\"Colormap: {cmap}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the colormapped image as is\n",
    "    Image.fromarray(ci).save(f\"depth_est_{cmap}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_preds = []\n",
    "num_steps = range(1, 21)\n",
    "\n",
    "for steps in num_steps:\n",
    "    result = pipe(image, num_inference_steps=steps)\n",
    "    vis = pipe.image_processor.visualize_depth(result.prediction)[0]\n",
    "    depth_preds.append(vis)\n",
    "\n",
    "# Show all depth visualizations\n",
    "fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    ax.imshow(depth_preds[idx])\n",
    "    ax.set_title(f\"Steps: {idx+1}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7920e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "# Compute pixel-wise differences between consecutive images\n",
    "diff_means = []\n",
    "\n",
    "for i in range(1, len(depth_preds)):\n",
    "    arr_curr = torch.tensor(np.array(depth_preds[i]).astype(float))\n",
    "    arr_prev = torch.tensor(np.array(depth_preds[i-1]).astype(float))\n",
    "    diff_mean = mse_loss(arr_curr, arr_prev).item()\n",
    "    diff_means.append(diff_mean)\n",
    "\n",
    "# Plot the mean pixel difference vs inference step\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, len(depth_preds) + 1), diff_means, marker='o')\n",
    "plt.xlabel('Inference Step')\n",
    "plt.ylabel('Mean Pixel Difference')\n",
    "plt.title('Mean Pixel Difference vs Inference Step')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = pipe(\n",
    "    image,\n",
    "    num_inference_steps=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb347e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pipe.image_processor.visualize_depth(depth.prediction)\n",
    "depth_16bit = pipe.image_processor.export_depth_to_16bit_png(depth.prediction)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.imshow(vis[0])\n",
    "ax1.set_title('Depth Visualization')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(depth_16bit[0], cmap='gray')\n",
    "ax2.set_title('Depth 16-bit PNG')\n",
    "ax2.axis('off')\n",
    "\n",
    "# fig.colorbar(ax1.imshow(vis[0]), ax=ax1, orientation='vertical')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
